base_model: mistralai/Mistral-7B-Instruct-v0.2

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj

training:
  batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  num_train_epochs: 3
  fp16: true
  logging_steps: 10
  save_strategy: epoch
